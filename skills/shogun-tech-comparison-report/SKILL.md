---
name: tech-comparison-report
description: 任意の2つの技術（フレームワーク、プロトコル、アーキテクチャパターン等）を入力として受け取り、両者の関係性分析・多軸比較・ユースケース分析・Mermaidフロー図を含む包括的な比較レポートを自動生成する。「RAGとMCPの関係を分析して」「SpringとNablarchを比較して」「GraphQLとgRPCの違いを整理して」「KubernetesとDockerの関係性を図解して」といった要望に対応する。また、OSSモデル（Embedding、LLM、Reranker等）の比較評価にも対応し、ベンチマークスコア・ライセンス・リソース使用量を比較して推奨モデルを提案する。「EmbeddingモデルのMTEBスコアを比較して」「商用利用可能なLLMを比較して」「○○タスク向けの最適モデルを推薦して」といった要望にも対応する。WebSearch + WebFetch + arXiv論文調査 + Mermaid図自動生成を駆使した技術比較・モデル評価スキル。
---

# Tech Comparison Report

## Overview

任意の2つの技術を入力として受け取り、以下を含む包括的な関係性分析・比較レポートをMarkdown形式で出力するスキル。

**出力に含まれる要素:**
- エグゼクティブサマリ（結論先行）
- 両技術の基礎概念と関係性の図解
- 多軸比較表（8軸以上）
- Mermaid図（関係性図、アーキテクチャ比較、シーケンス図、意思決定フローチャート）
- ユースケース分析（3件以上、各技術の貢献度マトリクス付き）
- 推奨アーキテクチャの提示
- 学術論文・技術ブログの引用
- 段階的導入ロードマップ

**本スキルの特徴:**
- 単なる機能比較に留まらず、2つの技術の**関係性**（補完/代替/依存/独立）を分析する
- 抽象的な比較ではなく、具体的なユースケースに基づく実践的な分析を行う
- 20以上のMermaid図・ASCIIアート図を含む視覚的なレポートを生成する
- arXiv論文やRFCなどの学術・標準文献を参照し、根拠を明確にする

**本スキルの原型:**
O-023（RAG x MCP関連性分析レポート）を汎用化したもの。1190行・20以上のMermaid/ASCII図を含むレポートを生成した実績に基づく。

**OSSモデル評価モード:**
本スキルはOSSモデル（Embedding、LLM、Reranker等）の比較評価にも対応する。モデル評価モードでは以下を提供する：
- MTEB/LMSYS等の公開ベンチマークスコア比較
- ライセンス（Apache-2.0、MIT、商用利用可否等）の整理
- リソース使用量（メモリ、GPU VRAM、推論速度）の比較
- タスク適性に基づく推奨モデルの提案
- コスト効率分析（品質/コスト比）

## When to Use

以下のいずれかに該当する場合にこのスキルを使用する：

- 「○○と△△の関係を分析して」
- 「○○と△△を比較して」
- 「○○と△△はどう違うのか整理して」
- 「○○と△△の使い分けを図解して」
- 「○○と△△を組み合わせたアーキテクチャを設計して」
- 「○○から△△への移行を検討するための比較レポートがほしい」
- 2つの技術の関係性（補完/代替/統合）を明確にしたい場合
- 技術選定の判断材料として、多軸での比較分析が必要な場合
- ステークホルダー向けに、図解入りの技術比較資料を作成したい場合

**トリガーキーワード**: 技術比較, 関係性分析, vs, 違い, 使い分け, 補完関係, 代替関係, 統合パターン, アーキテクチャ比較

**対象となる技術ペアの例:**
- フレームワーク同士: Spring vs Nablarch, React vs Vue.js, Django vs FastAPI
- プロトコル同士: REST vs GraphQL, gRPC vs REST, WebSocket vs SSE
- 補完技術: RAG vs MCP, Docker vs Kubernetes, Terraform vs Ansible
- アーキテクチャ: マイクロサービス vs モノリス, サーバーレス vs コンテナ
- データベース: PostgreSQL vs MySQL, Redis vs Memcached, MongoDB vs DynamoDB

### OSSモデル評価モード

以下のいずれかに該当する場合、OSSモデル評価モードで実行する：

- 「○○（Embedding/LLM/Reranker等）のOSSモデルを比較して」
- 「○○タスク向けの最適なモデルを推薦して」
- 「○○のMTEB/ベンチマークスコアを比較して」
- 「○○のライセンス・商用利用可否を調べて」
- 「○○のメモリ/GPU使用量を比較して」
- 「コスト効率の良いEmbeddingモデルを選定して」
- 「日本語対応のLLMを比較評価して」
- モデル選定の判断材料として、多軸での性能比較が必要な場合

**モデル評価トリガーキーワード**: OSSモデル, Embedding, LLM, Reranker, SLM, ベンチマーク, MTEB, LMSYS, HuggingFace, 推論コスト, VRAM, メモリ使用量, 商用利用, Apache-2.0, multilingual, 日本語対応

**対象モデルカテゴリの例:**
- Embeddingモデル: text-embedding-3, Jina-embeddings-v3, voyage-3, multilingual-e5, bge-m3
- LLM: Llama-3, Mistral, Qwen, Gemma, Phi, Command-R
- Reranker: bge-reranker, Jina-reranker, ms-marco-MiniLM
- Code専用: voyage-code-3, CodeLlama, DeepSeek-Coder, StarCoder
- 日本語特化: japanese-stablelm, ELYZA, Swallow, PLaMo

## Instructions

### Phase 1: 事前調査と関係性の仮説立案

2つの技術それぞれについてWeb検索で最新情報を収集し、両者の関係性の仮説を立てる。

#### Step 1.1: 技術Aの基礎情報収集

```
【検索パターン -- 並列実行可能】

1. 公式情報・概要
   WebSearch: "{技術A} とは 概要 公式"
   WebSearch: "{技術A} official documentation overview"

2. アーキテクチャ・設計思想
   WebSearch: "{技術A} architecture design philosophy"
   WebSearch: "{技術A} アーキテクチャ 設計思想"

3. 最新動向
   WebSearch: "{技術A} {current_year} updates new features"
   WebSearch: "{技術A} {current_year} 最新 動向"

4. 学術論文・RFC（該当する場合）
   WebSearch: "site:arxiv.org {技術A}"
   WebSearch: "{技術A} RFC specification"

【抽出項目】
- 正式名称・バージョン
- 開発元・コミュニティ
- 設計思想・目的
- 主要機能・特徴
- エコシステム概要
- ライセンス
```

#### Step 1.2: 技術Bの基礎情報収集

```
【検索パターン -- 技術Aと同様の並列実行】

技術Aと同じ検索パターンを技術Bに対して実行する。
技術AとBの検索は完全に独立しているため、全て並列実行可能。

【重要】
- 技術Aの検索結果を待たず、技術Bの検索も同時に開始する
- 合計8-12件のWebSearchを並列実行する
```

#### Step 1.3: 関係性の直接調査

```
【検索パターン -- 並列実行可能】

1. 直接比較記事
   WebSearch: "{技術A} vs {技術B} comparison {current_year}"
   WebSearch: "{技術A} {技術B} 比較 違い"

2. 統合・併用事例
   WebSearch: "{技術A} {技術B} integration together"
   WebSearch: "{技術A} {技術B} 統合 併用 組み合わせ"

3. 移行・置換事例
   WebSearch: "{技術A} to {技術B} migration"
   WebSearch: "{技術B} replace {技術A}"

4. 学術論文での比較
   WebSearch: "site:arxiv.org {技術A} {技術B}"

5. 両技術を扱うブログ記事
   WebSearch: "{技術A} {技術B} blog tutorial guide"

【主要記事はWebFetchで深掘り】
- 比較表を含む記事（3件以上）
- 統合アーキテクチャを解説する記事
- 学術論文のabstract
```

#### Step 1.4: 関係性の仮説立案

```
【関係性の4類型から仮説を立てる】

Type 1: 補完関係（Complementary）
  定義: 両者が異なる役割を果たし、組み合わせることで相乗効果が得られる
  例: RAG（知識検索）+ MCP（ツール提供）→ AIが「知って使う」仕組み
  例: Docker（コンテナ化）+ Kubernetes（オーケストレーション）
  判定基準:
    - 両者の主要機能が重複しない
    - 統合事例が多い
    - 「○○ with △△」の記事が多い

Type 2: 代替関係（Alternative）
  定義: 同じ問題を異なるアプローチで解決する競合関係
  例: React vs Vue.js, REST vs GraphQL
  判定基準:
    - 両者の主要機能が重複する
    - 「○○ vs △△」の記事が多い
    - ユーザーが両方を同時に採用することが稀

Type 3: 依存関係（Dependent）
  定義: 一方が他方の上位/下位レイヤーとして機能する
  例: TCP/IP の上に HTTP, フレームワーク上のライブラリ
  判定基準:
    - 一方が他方なしでは機能しない
    - 階層構造が明確
    - 「○○ on △△」の表現が多い

Type 4: 進化関係（Evolutionary）
  定義: 一方が他方の後継・発展形として位置づけられる
  例: REST → GraphQL, Docker Compose → Kubernetes
  判定基準:
    - 時系列で後発が先発の課題を解決している
    - 移行パスが文書化されている
    - 「Next generation of ○○」の表現が多い

【注意】
- 1つの類型に限定する必要はない。複合的な関係もある
  例: RAGとMCP は「補完関係」かつ「統合可能」
- 仮説は後のフェーズで検証・修正する
```

### Phase 2: 比較軸の定義と情報収集

#### Step 2.1: 標準比較軸（8軸）の情報収集

以下の8軸で両技術を比較するための情報を収集する。各軸について少なくとも1件のWebSearchを実行する。

```
【比較軸と検索パターン】

軸1: アーキテクチャ（Architecture）
  収集項目: 設計思想、構成要素、データフロー、プロトコル
  WebSearch: "{技術A} {技術B} architecture comparison"
  WebSearch: "{技術A} アーキテクチャ 構成要素"
  WebSearch: "{技術B} アーキテクチャ 構成要素"

軸2: ユースケース（Use Cases）
  収集項目: 主要な適用場面、対象ドメイン、業界事例
  WebSearch: "{技術A} use cases real world examples"
  WebSearch: "{技術B} use cases real world examples"

軸3: パフォーマンス（Performance）
  収集項目: レイテンシ、スループット、スケーラビリティ、リソース消費
  WebSearch: "{技術A} vs {技術B} performance benchmark"
  WebSearch: "{技術A} {技術B} scalability"

軸4: 学習コスト（Learning Curve）
  収集項目: 入門難易度、ドキュメント品質、チュートリアル充実度、前提知識
  WebSearch: "{技術A} learning curve difficulty"
  WebSearch: "{技術B} learning curve documentation"

軸5: 統合ポテンシャル（Integration Potential）
  収集項目: 他技術との連携容易性、エコシステム、プラグイン/拡張
  WebSearch: "{技術A} ecosystem plugins integrations"
  WebSearch: "{技術B} ecosystem plugins integrations"

軸6: 成熟度（Maturity）
  収集項目: リリース履歴、バージョン安定性、本番実績、エンタープライズ採用
  WebSearch: "{技術A} maturity production ready enterprise"
  WebSearch: "{技術B} maturity production ready enterprise"

軸7: コミュニティ（Community）
  収集項目: GitHub Stars, コントリビューター数, Stack Overflow質問数, 日本語情報量
  WebSearch: "{技術A} community size contributors github"
  WebSearch: "{技術B} community github stars"
  GitHub CLI: gh api repos/{owner}/{repo} で統計取得（可能な場合）

軸8: コスト（Cost）
  収集項目: ライセンス、運用コスト、SaaS料金、人件費（スキル希少性）
  WebSearch: "{技術A} pricing cost license"
  WebSearch: "{技術B} pricing cost license free"
```

#### Step 2.2: カスタム比較軸の追加（技術ペアに応じて）

```
【技術ペアの種類に応じた追加軸】

フレームワーク比較の場合:
  - 開発生産性（Development Productivity）
  - テスト容易性（Testability）
  - デプロイ方式（Deployment）

プロトコル比較の場合:
  - 標準化状況（Standardization）
  - セキュリティモデル（Security Model）
  - 後方互換性（Backward Compatibility）

データベース比較の場合:
  - クエリ言語（Query Language）
  - データモデル（Data Model）
  - 一貫性モデル（Consistency Model）
  - バックアップ・復旧（Backup/Recovery）

AI/ML技術比較の場合:
  - 精度・品質（Accuracy/Quality）
  - 説明可能性（Explainability）
  - データ要件（Data Requirements）
```

#### Step 2.3: arXiv論文の調査

```
【arXiv論文の調査手順】

1. 論文検索
   WebSearch: "site:arxiv.org {技術A} {技術B}"
   WebSearch: "arxiv {技術A} {技術B} {current_year}"

2. 関連論文が見つかった場合
   WebFetch で abstract を取得:
   URL: https://arxiv.org/abs/{論文ID}
   抽出項目:
   - タイトル
   - 著者
   - 公開日
   - abstract（要約）
   - 主要な定量結果（精度○%向上、コスト○%削減等）

3. 引用フォーマット
   レポート内では以下の形式で引用する:

   > {主要な知見の要約}（{著者} et al., arXiv:{論文ID}）

   参考情報セクションでは:
   | {タイトル} | https://arxiv.org/abs/{論文ID} | {概要} |

【実践知見 -- O-023の実績】
- RAG-MCP論文（arXiv:2505.03275）を発見
- 「RAGによるMCPツール選択でプロンプトトークン75%削減・精度3倍向上」の定量結果を引用
- 学術論文は比較の客観性・信頼性を大幅に向上させる
```

### Phase 3: 関係性の分析と図解

#### Step 3.1: 関係性の確定

```
【Phase 1の仮説をPhase 2の情報で検証する】

検証チェックリスト:
□ 両技術の主要機能の重複度を0-100%で評価したか
□ 統合事例と競合事例の比率を確認したか
□ 学術論文での位置づけを確認したか（見つかった場合）
□ 実際のプロジェクトでの併用/選択の傾向を確認したか

関係性の結論フォーマット:
「{技術A}と{技術B}は{関係性の類型}にある。{理由の要約}」

例: 「RAGとMCPは補完関係にある。RAGは"知る"仕組み、MCPは"使う"仕組みであり、
     機能の重複はほぼなく、統合により相乗効果が得られる」
```

#### Step 3.2: アナロジーの作成

```
【関係性を直感的に理解できるアナロジーを作成する】

作成基準:
- 技術に詳しくない人でも理解できるアナロジー
- 両技術の本質的な違いと関係性を正確に表現する
- 1つの統一的なメタファーで両者を説明する

例（O-023実績）:
  RAG = 図書館の「蔵書検索システム」（知識の検索）
  MCP = 図書館の「窓口サービス」（機能の提供）
  統合 = 図書館の「司書」（検索して提供する）

テンプレート:
  {技術A} = {アナロジー対象}の「{役割A}」（{本質的機能}）
  {技術B} = {アナロジー対象}の「{役割B}」（{本質的機能}）
  統合 = {アナロジー対象}の「{統合的役割}」（{統合的機能}）
```

#### Step 3.3: 関係性図の作成（Mermaid）

```
【Mermaid関係性図テンプレート】

パターン1: 補完関係の場合

graph LR
    subgraph {技術A}の領域
        A1[{技術A}の機能1]
        A2[{技術A}の機能2]
    end

    subgraph {技術B}の領域
        B1[{技術B}の機能1]
        B2[{技術B}の機能2]
    end

    subgraph 統合レイヤー
        C[統合ポイント]
    end

    A1 --> C
    A2 --> C
    B1 --> C
    B2 --> C
    C --> D[統合効果]


パターン2: 代替関係の場合

graph TD
    P[共通の問題/目的]
    P --> A[{技術A}のアプローチ]
    P --> B[{技術B}のアプローチ]
    A --> A1[{技術A}の強み]
    A --> A2[{技術A}の弱み]
    B --> B1[{技術B}の強み]
    B --> B2[{技術B}の弱み]


パターン3: 依存関係の場合

graph BT
    L1[{下位技術}] --> L2[{上位技術}]
    L2 --> L3[アプリケーション]

    subgraph レイヤー構造
        L1
        L2
        L3
    end


パターン4: 進化関係の場合

graph LR
    A[{先発技術}] -->|課題| B[{後発技術}]
    B -->|解決| C[改善点]
    A -->|残存価値| D[引き続き有効な場面]
```

### Phase 4: 比較表とMermaid図の生成

#### Step 4.1: メイン比較表の作成

```
【メイン比較表テンプレート】

| 評価観点 | {技術A} | {技術B} | {技術A}+{技術B}統合 |
|---------|---------|---------|-------------------|
| **アーキテクチャ** | {概要} | {概要} | {統合時の概要} |
| **ユースケース** | {主要用途} | {主要用途} | {統合ユースケース} |
| **パフォーマンス** | {評価} | {評価} | {統合時の評価} |
| **学習コスト** | {高/中/低} | {高/中/低} | {統合時のコスト} |
| **統合ポテンシャル** | {エコシステム} | {エコシステム} | {相互統合} |
| **成熟度** | {評価} | {評価} | {統合事例の成熟度} |
| **コミュニティ** | {規模} | {規模} | {統合コミュニティ} |
| **コスト** | {概算} | {概算} | {統合コスト} |
| **{技術ペア固有の軸}** | ... | ... | ... |

【記号の使い方】
- ◎: 非常に優れている / 最適
- ○: 優れている / 適している
- △: 一部制約あり / 条件付き
- ✗: 対応していない / 不向き
- ★: 特筆すべき強み
```

#### Step 4.2: アーキテクチャ比較図の作成

```
【ASCIIアーキテクチャ図テンプレート】

各技術のアーキテクチャをASCIIアートで並べて比較する。

パターンA単体:
┌─────────────────────────────────────────────┐
│               {技術A}単体アーキテクチャ         │
│                                              │
│  ┌──────────┐                                │
│  │ ユーザー   │                                │
│  └─────┬────┘                                │
│        │                                     │
│        ▼                                     │
│  ┌─────────────────────────────────────┐    │
│  │     {技術A}の主要コンポーネント       │    │
│  │  ┌────────┐  ┌────────┐             │    │
│  │  │コンポ1  │  │コンポ2  │             │    │
│  │  └────────┘  └────────┘             │    │
│  └─────────────────────────────────────┘    │
└─────────────────────────────────────────────┘

パターンB単体:
（同様の構造で技術Bを記述）

統合パターン:
┌─────────────────────────────────────────────┐
│          {技術A}+{技術B}統合アーキテクチャ      │
│                                              │
│  ┌─────────────────────────────────────┐    │
│  │  {技術B}（外殻）                      │    │
│  │  ┌────────────────────────────┐     │    │
│  │  │  {技術A}（内部エンジン）    │     │    │
│  │  └────────────────────────────┘     │    │
│  └─────────────────────────────────────┘    │
└─────────────────────────────────────────────┘
```

#### Step 4.3: シーケンス図の作成（Mermaid）

```
【シーケンス図テンプレート -- 各パターン最低1つ作成】

パターンA: 技術A単体のシーケンス

sequenceDiagram
    participant User as ユーザー
    participant A as {技術A}
    participant Store as データストア

    User->>{技術A}: リクエスト
    {技術A}->>Store: データ取得
    Store-->>{技術A}: レスポンス
    {技術A}-->>User: 結果


パターンB: 技術B単体のシーケンス
（同様の構造で技術Bのフローを記述）

パターンC: 統合パターンのシーケンス

sequenceDiagram
    participant User as ユーザー
    participant B as {技術B}
    participant A as {技術A}
    participant Store as データストア

    User->>{技術B}: リクエスト
    {技術B}->>{技術A}: 内部処理委譲
    {技術A}->>Store: データ取得
    Store-->>{技術A}: レスポンス
    {技術A}-->>{技術B}: 処理結果
    {技術B}-->>User: 統合結果
```

#### Step 4.4: 意思決定フローチャートの作成

```
【意思決定フローチャートテンプレート -- ASCIIアート】

技術選定を支援する意思決定フロー:

              {判断基準1}を重視
                  │
                  │
    ┌─────────────┼─────────────┐
    │             │             │
    │  {技術A}     │ {技術A}+{技術B}│
    │  単体        │  統合        │
    │             │             │
    │  {適用場面A} │  {適用場面C}  │
{基準2}───────────┼──────────── {基準2}
不要              │             必要
    │             │             │
    │  {適用場面D} │  {技術B}     │
    │             │  単体        │
    │             │             │
    │             │  {適用場面B}  │
    └─────────────┼─────────────┘
                  │
              {判断基準3}を重視

使い方:
- 2軸（基準1: 縦軸、基準2: 横軸）で4象限に分類
- 各象限に最適な技術パターンを配置
- ユーザーの要件に基づき、適切な象限を選択可能にする
```

#### Step 4.5: Mermaid統合パターン図

```
【統合パターンの網羅的図解 -- ASCIIアート】

技術A+Bの統合パターンが複数ある場合、全パターンを図解する。

┌──────────────────────────────────────────────────────────────┐
│                                                               │
│  パターン1: {技術A} inside {技術B}                              │
│  ─────────────────────────                                    │
│  {説明文}                                                     │
│                                                               │
│  ┌─────────────────────────────────────┐                     │
│  │         {技術B}                      │                     │
│  │  ┌─────────────────────────┐        │                     │
│  │  │   {技術A}                │        │                     │
│  │  └─────────────────────────┘        │                     │
│  └─────────────────────────────────────┘                     │
│                                                               │
│  パターン2: {技術A} for {技術B}                                 │
│  ──────────────────────────────                                │
│  {説明文}                                                     │
│                                                               │
│  ┌──────────┐    ┌───────────────┐    ┌──────────────┐      │
│  │ {技術A}   │───▶│ 中間処理       │───▶│ {技術B}       │      │
│  └──────────┘    └───────────────┘    └──────────────┘      │
│                                                               │
│  パターン3: {技術A} + {技術B} 並列利用                           │
│  ──────────────────────────────                                │
│  {説明文}                                                     │
│                                                               │
│  ┌──────────┐                                                │
│  │ {技術A}   │──────┐                                         │
│  └──────────┘      ├───▶ 統合ポイント                         │
│  ┌──────────┐      │                                         │
│  │ {技術B}   │──────┘                                         │
│  └──────────┘                                                │
│                                                               │
└──────────────────────────────────────────────────────────────┘
```

### Phase 5: ユースケース分析

#### Step 5.1: ユースケースの選定（3件以上）

```
【ユースケース選定基準】

以下の基準でユースケースを選定する:

1. 代表性: その技術ペアの典型的な使い方を示す
2. 差異の明確性: 技術A単体/B単体/統合の違いが明確に出る
3. 実用性: 実際のプロジェクトで発生しうるシナリオ
4. 多様性: 異なる業界・規模・目的を含む

選定するユースケース数:
- 最低3件
- 推奨5-7件（O-023では7件のユースケースを分析）

各ユースケースで記述する内容:
- ユースケース名
- シナリオ説明（2-3文）
- 技術Aの担当範囲
- 技術Bの担当範囲
- 統合時のフロー（Mermaidシーケンス図）
- 3パターン比較表（技術A単体 / 技術B単体 / 統合）
```

#### Step 5.2: 各ユースケースの詳細分析

```
【ユースケース詳細テンプレート】

### ユースケースN: {ユースケース名}

{シナリオ説明: このユースケースが何を達成するか、誰が使うか}

#### {技術A}の担当
- {機能1}: {具体的な役割}
- {機能2}: {具体的な役割}

#### {技術B}の担当
- {機能1}: {具体的な役割}
- {機能2}: {具体的な役割}

#### {技術A}+{技術B}連携フロー

（Mermaidシーケンス図を挿入）

#### パターン比較

| 要素 | {技術A}単体 | {技術B}単体 | {技術A}+{技術B}統合 |
|------|-----------|-----------|-------------------|
| {評価項目1} | {評価} | {評価} | {評価} |
| {評価項目2} | {評価} | {評価} | {評価} |
| 総合品質 | ○ | ○ | **◎** |
```

#### Step 5.3: 貢献度マトリクスの作成

```
【ユースケース横断 -- 貢献度マトリクステンプレート】

| ユースケース | {技術A}の貢献度 | {技術B}の貢献度 | 統合効果 |
|-------------|:--------------:|:--------------:|:-------:|
| UC1: {名前}  | ★★★★★         | ★★★★☆         | **極高** |
| UC2: {名前}  | ★★★★☆         | ★★★★★         | **極高** |
| UC3: {名前}  | ★★★★★         | ★★★☆☆         | **高**   |

★の基準:
  ★★★★★: そのユースケースに不可欠。代替手段がない
  ★★★★☆: 非常に有用。なくても成立するが品質が大幅に低下
  ★★★☆☆: 有用。補助的な役割
  ★★☆☆☆: 限定的な貢献
  ★☆☆☆☆: ほぼ不要

統合効果の基準:
  **極高**: 統合により単体の2倍以上の効果
  **高**:   統合により明確な改善
  **中**:   統合のメリットはあるが限定的
  **低**:   統合の必要性が薄い

【分析コメント -- マトリクスの後に必ず記述】
「{技術A}はすべてのユースケースで{主要貢献}として不可欠。
 {技術B}は{主要貢献}が求められるユースケースで特に貢献度が高い。
 両者を統合することで、{統合効果の要約}。」
```

### Phase 6: 推奨アーキテクチャと導入ロードマップ

#### Step 6.1: 推奨アーキテクチャの提示

```
【推奨アーキテクチャの構成要素】

1. 推奨パターン名: 明確な名前を付ける
   例: 「RAG-enhanced MCPサーバー」「GraphQL Gateway + REST Backend」

2. アーキテクチャ図: ASCIIアートで詳細な構成を記述
   - 全レイヤーを網羅（プレゼンテーション/ビジネスロジック/データ）
   - コンポーネント間の通信プロトコルを明記
   - 外部依存関係を明示

3. 技術スタック推奨表:
   | レイヤー | 技術 | 根拠 |
   |---------|------|------|
   | ... | ... | ... |

4. 技術的考慮事項:
   | 考慮事項 | 詳細 | 対応方針 |
   |---------|------|---------|
   | ... | ... | ... |
```

#### Step 6.2: 段階的導入ロードマップ

```
【ロードマップテンプレート -- 4フェーズ】

Phase 1: 基盤構築
├── {技術A or B}の基本セットアップ
├── 最小限の機能で動作確認
└── 評価基準の策定

    │
    ▼

Phase 2: コア機能実装
├── もう一方の技術の統合
├── 主要ユースケース3件の実装
└── 統合テスト

    │
    ▼

Phase 3: 品質向上・拡充
├── 追加ユースケースの実装
├── パフォーマンスチューニング
├── セキュリティ対応
└── ユーザーテスト

    │
    ▼

Phase 4: 高度化・エンタープライズ対応
├── スケーリング
├── 運用自動化
├── 高度な統合機能
└── ドキュメント整備
```

### Phase 7: レポート生成

#### Step 7.1: レポート全体構成

「Output Format」セクションのテンプレートに従い、以下の構成でレポートを作成する。

```
1. エグゼクティブサマリ
2. 基礎概念と関係性（Mermaid図3枚以上）
3. 多軸比較表（8軸以上）
4. ユースケース分析（3件以上、各Mermaidシーケンス図付き）
5. 推奨アーキテクチャ
6. 参考情報
```

#### Step 7.2: 品質チェックリスト（レポート完成前に必ず確認）

```
【必須チェック項目】

□ 構造チェック
  □ エグゼクティブサマリが結論先行になっているか
  □ 目次が正しくリンクしているか
  □ 全セクションに内容が記述されているか

□ 図解チェック
  □ Mermaid図が5枚以上含まれているか
  □ ASCIIアーキテクチャ図が3枚以上含まれているか
  □ 意思決定フローチャートが含まれているか
  □ 全ての図に説明テキストが付いているか
  □ Mermaid構文が正しいか（閉じタグ忘れ等がないか）

□ 比較の公平性チェック
  □ 両技術の長所と短所を公平に記述しているか
  □ 一方の技術に偏った推奨になっていないか
  □ 統合パターンだけでなく、単体利用の適切な場面も記述しているか

□ 情報の信頼性チェック
  □ 全ての主要主張にソースURL or 論文引用が付いているか
  □ 推測と事実が区別されているか
  □ 定量データ（Stars数、ベンチマーク等）の出典が明記されているか
  □ 情報の鮮度（取得日/公開日）が記載されているか

□ ユースケースチェック
  □ 3件以上のユースケースが分析されているか
  □ 各ユースケースにシーケンス図が含まれているか
  □ 貢献度マトリクスが作成されているか
  □ 各ユースケースで3パターン（A単体/B単体/統合）の比較があるか

□ 実用性チェック
  □ 推奨アーキテクチャが具体的か
  □ 段階的導入ロードマップがあるか
  □ 技術スタック推奨表があるか
  □ 読者が技術選定の判断を下せるだけの情報があるか

□ レポート品質チェック
  □ 500行以上あるか（目標: 800-1200行）
  □ 日本語の表記ゆれがないか
  □ Markdownの見出しレベルが正しいか
  □ テーブルのカラム数が各行で一致しているか
```

---

### Phase M: OSSモデル評価モード（モデル比較専用フロー）

OSSモデル（Embedding、LLM、Reranker等）の比較評価を行う場合、Phase 1-7の代わりに以下のPhase M1-M6を実行する。

#### Phase M1: モデルカテゴリと評価対象の特定

```
【入力パラメータの解析】

1. モデルカテゴリの特定
   - Embedding: 文書/クエリのベクトル化（RAG検索、類似度計算）
   - LLM: テキスト生成（チャット、要約、翻訳）
   - Reranker: 検索結果の再順位付け
   - Code専用: コード生成・補完・説明
   - Multimodal: 画像+テキスト処理

2. 評価対象モデルの決定
   - ユーザー指定がある場合: 指定モデルを評価
   - 指定がない場合: カテゴリ上位5-10モデルを自動選定

3. ユースケースの明確化
   - 用途: RAG, チャット, コード生成, 翻訳, 要約 等
   - 言語要件: 英語のみ, 日本語対応, multilingual
   - デプロイ環境: クラウドAPI, オンプレミス, エッジ
```

#### Phase M2: ベンチマークスコアの収集

```
【検索パターン -- 並列実行可能】

1. MTEB（Massive Text Embedding Benchmark）※Embeddingモデル
   WebSearch: "MTEB leaderboard {model_name}"
   WebSearch: "MTEB benchmark {model_category} {current_year}"
   WebFetch: https://huggingface.co/spaces/mteb/leaderboard

2. LMSYS Chatbot Arena（LLM）
   WebSearch: "LMSYS chatbot arena leaderboard {current_year}"
   WebSearch: "{model_name} LMSYS ELO score"
   WebFetch: https://chat.lmsys.org/?leaderboard

3. Open LLM Leaderboard（LLM）
   WebSearch: "Open LLM Leaderboard {model_name}"
   WebFetch: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

4. タスク固有ベンチマーク
   WebSearch: "{model_name} benchmark {task_name}"
   WebSearch: "{model_name} evaluation {language} {current_year}"

5. 日本語ベンチマーク（日本語対応モデルの場合）
   WebSearch: "{model_name} 日本語 ベンチマーク"
   WebSearch: "Japanese LLM benchmark {model_name}"
   WebSearch: "JGLUE {model_name}"

【抽出項目】
- 総合スコア / ランキング
- タスク別スコア（検索、分類、クラスタリング等）
- 言語別スコア（日本語スコアがあれば）
- ベンチマーク取得日（情報の鮮度確認）
```

#### Phase M3: ライセンス・商用利用可否の調査

```
【検索パターン -- 並列実行可能】

1. HuggingFaceモデルカード
   WebFetch: https://huggingface.co/{org}/{model_name}
   抽出: License, Commercial use, 利用規約

2. GitHubリポジトリ
   WebSearch: "{model_name} github license"
   gh api repos/{owner}/{repo} （可能な場合）

3. 公式ドキュメント
   WebSearch: "{model_name} license terms commercial"
   WebSearch: "{model_name} 商用利用 ライセンス"

【ライセンス分類】
- 完全オープン: Apache-2.0, MIT, BSD → 商用利用自由
- 条件付きオープン: Llama 3 Community License → 月間アクティブユーザー制限等
- 研究目的限定: CC-BY-NC, 一部独自ライセンス → 商用利用不可
- 要確認: 独自ライセンス → 個別に利用規約を確認

【出力テーブル形式】
| モデル | ライセンス | 商用利用 | 制約事項 |
|--------|-----------|---------|---------|
| ... | Apache-2.0 | ◎ 自由 | なし |
| ... | Llama 3 | ○ 条件付き | MAU 7億未満 |
| ... | CC-BY-NC | ✗ 不可 | 研究目的のみ |
```

#### Phase M4: リソース使用量の調査

```
【検索パターン -- 並列実行可能】

1. モデルサイズ・パラメータ数
   WebSearch: "{model_name} parameters size"
   WebFetch: HuggingFaceモデルカード

2. メモリ/VRAM使用量
   WebSearch: "{model_name} VRAM memory requirements"
   WebSearch: "{model_name} GPU memory inference"

3. 推論速度
   WebSearch: "{model_name} inference speed latency"
   WebSearch: "{model_name} tokens per second"

4. 量子化バージョン
   WebSearch: "{model_name} GGUF quantization"
   WebSearch: "{model_name} AWQ GPTQ INT4 INT8"

【収集項目】
- パラメータ数（7B, 13B, 70B等）
- FP16時のVRAM使用量
- 量子化時のVRAM使用量（INT4, INT8）
- 推論速度（tokens/sec または ms/token）
- コンテキスト長（最大トークン数）
- Embedding次元数（Embeddingモデルの場合）

【リソース使用量テーブル形式】
| モデル | パラメータ | VRAM (FP16) | VRAM (INT4) | 速度 | コンテキスト長 |
|--------|-----------|-------------|-------------|------|---------------|
| ... | 7B | 14GB | 4GB | 50 tok/s | 8K |
```

#### Phase M5: コスト効率分析

```
【コスト効率の計算】

1. API利用コスト（商用APIの場合）
   - 入力トークン単価
   - 出力トークン単価
   - 月間想定コスト（使用量シナリオ別）

2. セルフホスティングコスト（オンプレミスの場合）
   - 必要GPU（A100, H100, RTX 4090等）
   - GPU時間単価（クラウドGPU利用時）
   - 月間インフラコスト概算

3. 品質/コスト比の算出
   - ベンチマークスコア ÷ コスト = コスト効率指数
   - 用途別の最適コストパフォーマンスモデルを特定

【コスト効率テーブル形式】
| モデル | ベンチマーク | 月間コスト概算 | コスト効率 | 推奨用途 |
|--------|-------------|---------------|-----------|---------|
| ... | 85.2 | $50 | ★★★★★ | 高頻度RAG |
| ... | 92.1 | $500 | ★★★☆☆ | 精度重視 |
```

#### Phase M6: 推奨モデルの提案とレポート生成

```
【推奨ロジック】

1. ユースケース別推奨
   用途ごとに最適モデルを1-3件提案:
   - 「精度重視」: ベンチマークスコア最高のモデル
   - 「コスト重視」: コスト効率指数最高のモデル
   - 「バランス型」: 精度とコストのバランスが良いモデル
   - 「日本語特化」: 日本語ベンチマークスコア最高のモデル
   - 「エッジ向け」: 軽量で低リソースのモデル

2. 制約条件別推奨
   - 商用利用必須 → ライセンスフィルタ適用
   - VRAM制限あり → リソースフィルタ適用
   - オンプレミス必須 → セルフホスティング可能モデルに限定

3. 総合推奨
   | 推奨順位 | モデル | 推奨理由 |
   |---------|--------|---------|
   | 1位 | ... | {理由} |
   | 2位 | ... | {理由} |
   | 3位 | ... | {理由} |

【レポート生成】
Phase M6完了後、「OSSモデル評価レポート」セクションのOutput Formatに従ってレポートを生成する。
```

#### Phase M7: モデル評価レポートの品質チェック

```
【必須チェック項目 -- モデル評価モード】

□ ベンチマークデータ
  □ MTEB/LMSYS等の公開ベンチマークを引用しているか
  □ ベンチマークの取得日が明記されているか
  □ 複数のベンチマークを参照しているか（1つだけに依存していないか）

□ ライセンス情報
  □ 全モデルのライセンスが明記されているか
  □ 商用利用可否が明確か
  □ 制約事項（MAU制限等）が記載されているか

□ リソース情報
  □ VRAM使用量が記載されているか
  □ 量子化オプションが検討されているか
  □ 推論速度の目安が記載されているか

□ 推奨の妥当性
  □ 推奨理由が明確か
  □ ユースケース別の推奨があるか
  □ コスト効率の観点が含まれているか
  □ 制約条件（ライセンス、リソース）を満たしているか

□ 公平性
  □ 特定モデル/ベンダーに偏っていないか
  □ OSSモデルと商用APIの両方を検討しているか（該当する場合）
```

## Input Format

スキル実行時に以下のパラメータを指定する。

```yaml
# 必須パラメータ
technology_a: "RAG"                              # 比較対象の技術A
technology_b: "MCP"                              # 比較対象の技術B
output_path: "output/O-023_rag_mcp_analysis.md"  # 出力ファイルパス

# オプションパラメータ
context: "Nablarch開発支援"            # 比較の文脈（特定ドメイン/プロジェクト）
relationship_hypothesis: "complementary" # 関係性の初期仮説（complementary / alternative / dependent / evolutionary）
depth: "standard"                      # quick / standard / thorough（デフォルト: standard）
comparison_axes:                       # 追加の比較軸
  - "セキュリティモデル"
  - "データガバナンス"
use_cases:                             # 分析してほしいユースケース
  - "ハンドラキュー自動設計"
  - "コード生成"
  - "トラブルシューティング"
github_repos:                          # 統計取得対象のGitHubリポジトリ
  technology_a:
    - owner: "langchain-ai"
      repo: "langchain"
  technology_b:
    - owner: "modelcontextprotocol"
      repo: "specification"
arxiv_search: true                     # arXiv論文を検索するか（デフォルト: true）
include_roadmap: true                  # 導入ロードマップを含めるか（デフォルト: true）
language: "ja"                         # 出力言語（デフォルト: ja）
```

### パラメータ説明

| パラメータ | 必須 | デフォルト | 説明 |
|----------|------|----------|------|
| `technology_a` | ✅ | -- | 比較対象の技術A。正式名称を推奨 |
| `technology_b` | ✅ | -- | 比較対象の技術B。正式名称を推奨 |
| `output_path` | ✅ | -- | 出力Markdownファイルのパス |
| `context` | -- | `""` | 比較の文脈。特定プロジェクトやドメインを指定すると、ユースケースがそれに特化する |
| `relationship_hypothesis` | -- | `""` | 関係性の初期仮説。空の場合はPhase 1で自動判定 |
| `depth` | -- | `standard` | 調査深度 |
| `comparison_axes` | -- | `[]` | 標準8軸に加える追加比較軸 |
| `use_cases` | -- | `[]` | 分析対象のユースケース。空の場合は自動選定 |
| `github_repos` | -- | `{}` | GitHub統計を取得するリポジトリ |
| `arxiv_search` | -- | `true` | arXiv論文を検索するか |
| `include_roadmap` | -- | `true` | 導入ロードマップを含めるか |
| `language` | -- | `ja` | 出力言語 |

### OSSモデル評価モード用パラメータ

```yaml
# モデル評価モードの場合
evaluation_mode: "oss_model"               # "tech_comparison" (デフォルト) | "oss_model"
model_category: "embedding"                # embedding | llm | reranker | code | multimodal
target_models:                             # 評価対象モデル（省略時は上位モデル自動選定）
  - "text-embedding-3-large"
  - "jina-embeddings-v3"
  - "voyage-3"
  - "bge-m3"
  - "multilingual-e5-large-instruct"
use_case: "RAG検索"                        # 想定ユースケース
language_requirements: "japanese"          # english | japanese | multilingual | any
deployment_target: "cloud_api"             # cloud_api | on_premise | edge
constraints:                               # 制約条件
  commercial_use: true                     # 商用利用必須か
  max_vram_gb: 24                          # 最大VRAM（GB）
  max_cost_per_month_usd: 500              # 月間最大コスト（USD）
```

| パラメータ | 必須 | デフォルト | 説明 |
|----------|------|----------|------|
| `evaluation_mode` | -- | `tech_comparison` | 評価モード。`oss_model` でモデル評価モード |
| `model_category` | ✅* | -- | モデルカテゴリ（モデル評価モード時は必須） |
| `target_models` | -- | `[]` | 評価対象モデル。省略時は自動選定 |
| `use_case` | -- | `""` | 想定ユースケース |
| `language_requirements` | -- | `any` | 言語要件 |
| `deployment_target` | -- | `cloud_api` | デプロイ先 |
| `constraints.commercial_use` | -- | `false` | 商用利用必須か |
| `constraints.max_vram_gb` | -- | `null` | 最大VRAM制約 |
| `constraints.max_cost_per_month_usd` | -- | `null` | 月間コスト上限 |

### 深度別の処理範囲

| Phase | quick | standard | thorough |
|-------|-------|----------|----------|
| 1. 事前調査 | 主要検索のみ（4-6件） | 全検索パターン（10-15件） | 全検索+論文深掘り（20件以上） |
| 2. 比較軸 | 標準8軸のみ | 8軸+カスタム軸2-3 | 8軸+カスタム軸5以上 |
| 3. 関係性分析 | 関係性図1枚 | 関係性図+統合パターン図 | 全パターン網羅 |
| 4. 比較表・図 | メイン比較表+シーケンス1枚 | 比較表+シーケンス3枚+意思決定フロー | 全図解（20枚以上） |
| 5. ユースケース | 3件 | 5件 | 7件以上 |
| 6. 推奨 | 簡易推奨 | 推奨+ロードマップ | 推奨+ロードマップ+コスト試算 |
| 7. レポート | 300-500行 | 600-900行 | 900-1200行以上 |

## Output Format

### ファイル構造テンプレート

```markdown
# {技術A}と{技術B}の関連性分析 -- {コンテキスト}レポート

> **作成日**: YYYY-MM-DD
> **タスクID**: {task_id}
> **作成者**: {agent_name}（ペルソナ: 戦略アナリスト / ソリューションアーキテクト）
> **プロジェクト**: {project}
> **ステータス**: 完了

---

## 目次

1. [エグゼクティブサマリ](#1-エグゼクティブサマリ)
2. [{技術A}と{技術B}の基礎概念と関係性](#2-{id_a}と{id_b}の基礎概念と関係性)
3. [多軸比較](#3-多軸比較)
4. [ユースケース分析](#4-ユースケース分析)
5. [推奨アーキテクチャ](#5-推奨アーキテクチャ)
6. [参考情報](#6-参考情報)

---

## 1. エグゼクティブサマリ

### 結論

**{技術A}と{技術B}は{関係性の結論}。** {1-2文の要約}

| 技術 | 役割 | {コンテキスト}での位置づけ |
|------|------|-------------------------|
| **{技術A}** | {役割} | {位置づけ} |
| **{技術B}** | {役割} | {位置づけ} |
| **{技術A}+{技術B}** | {統合の役割} | {統合の位置づけ} |

### 要点

1. **{技術A}単体**: {評価要約}
2. **{技術B}単体**: {評価要約}
3. **{技術A}+{技術B}統合**: {評価要約}
4. {定量データがあれば（論文引用等）}

---

## 2. {技術A}と{技術B}の基礎概念と関係性

### 2.1 {技術A}とは

{概要説明}

```mermaid
graph LR
    subgraph {技術A}のアーキテクチャ
        ...
    end
```

**特徴**:
- 情報フロー: ...
- データ: ...
- 強み: ...
- 弱み: ...

### 2.2 {技術B}とは

{概要説明}

```mermaid
graph LR
    subgraph {技術B}のアーキテクチャ
        ...
    end
```

**特徴**:
- 情報フロー: ...
- データ: ...
- 強み: ...
- 弱み: ...

### 2.3 両者の関係性 -- 「{アナロジー}」のメタファー

```
{ASCIIアートでの関係性図}
```

### 2.4 技術的な位置づけの比較

| 観点 | {技術A} | {技術B} |
|------|---------|---------|
| **役割** | ... | ... |
| **データアクセス** | ... | ... |
| **情報フロー** | ... | ... |
| **標準化** | ... | ... |
| **エコシステム** | ... | ... |
| **関係性** | → {技術B}の「{役割}」として機能 | → {技術A}の「{役割}」として機能 |

### 2.5 統合パターン概要

```
{ASCIIアートでの統合パターン図（Phase 4.5のテンプレート使用）}
```

---

## 3. 多軸比較

### 3.1 パターンA: {技術A}単体

{概要説明}

```mermaid
sequenceDiagram
    ...
```

**アーキテクチャ図**:
```
{ASCIIアート}
```

### 3.2 パターンB: {技術B}単体

{概要説明}

```mermaid
sequenceDiagram
    ...
```

**アーキテクチャ図**:
```
{ASCIIアート}
```

### 3.3 パターンC: {技術A}+{技術B}統合

{概要説明}

```mermaid
sequenceDiagram
    ...
```

**アーキテクチャ図**:
```
{ASCIIアート}
```

### 3.4 多軸比較表

| 評価観点 | {技術A}単体 | {技術B}単体 | {技術A}+{技術B}統合 |
|---------|-----------|-----------|-------------------|
| **アーキテクチャ** | ... | ... | ... |
| **ユースケース** | ... | ... | ... |
| **パフォーマンス** | ... | ... | ... |
| **学習コスト** | ... | ... | ... |
| **統合ポテンシャル** | ... | ... | ... |
| **成熟度** | ... | ... | ... |
| **コミュニティ** | ... | ... | ... |
| **コスト** | ... | ... | ... |

### 3.5 適用場面の整理

```
{意思決定フローチャート（ASCIIアート）}
```

---

## 4. ユースケース分析

### 4.1 ユースケース1: {名前}

{シナリオ説明}

#### {技術A}の担当
- ...

#### {技術B}の担当
- ...

#### {技術A}+{技術B}連携フロー

```mermaid
sequenceDiagram
    ...
```

#### 各技術の貢献度

```
{ASCIIアートでの処理フロー図}
```

---

### 4.2 ユースケース2: {名前}
{（同様の構造）}

---

### 4.3 ユースケース3: {名前}
{（同様の構造）}

---

{以降、ユースケースの数だけ繰り返し}

### 4.N ユースケース横断 -- 貢献度マトリクス

| ユースケース | {技術A}の貢献度 | {技術B}の貢献度 | 統合効果 |
|-------------|:--------------:|:--------------:|:-------:|
| ... | ★★★★★ | ★★★★☆ | **極高** |

**分析**: {マトリクスの総括}

---

## 5. 推奨アーキテクチャ

### 5.1 推奨: {推奨パターン名}

{推奨理由}

```
{ASCIIアートでの詳細アーキテクチャ図}
```

### 5.2 段階的構築ロードマップ

```
Phase 1: {名前}
├── ...

    │
    ▼

Phase 2: {名前}
├── ...

    │
    ▼

Phase 3: {名前}
├── ...

    │
    ▼

Phase 4: {名前}
├── ...
```

### 5.3 技術スタック推奨

| レイヤー | 技術 | 根拠 |
|---------|------|------|
| ... | ... | ... |

### 5.4 技術的考慮事項

| 考慮事項 | 詳細 | 対応方針 |
|---------|------|---------|
| ... | ... | ... |

---

## 6. 参考情報

### 学術・技術文献

| リソース | URL | 概要 |
|---------|-----|------|
| ... | ... | ... |

### 実装事例

| リソース | URL | 概要 |
|---------|-----|------|
| ... | ... | ... |

### プロジェクト既存資料（該当する場合）

| リソース | パス | 概要 |
|---------|------|------|
| ... | ... | ... |

### 公式仕様・ドキュメント

| リソース | URL |
|---------|-----|
| ... | ... |

---

*本レポートは{agent_name}が戦略アナリスト / ソリューションアーキテクトとして調査・作成したものである。*
```

### OSSモデル評価レポートテンプレート（evaluation_mode: oss_model の場合）

```markdown
# {model_category}モデル比較評価レポート -- {use_case}

> **作成日**: YYYY-MM-DD
> **タスクID**: {task_id}
> **作成者**: {agent_name}（ペルソナ: MLエンジニア / ソリューションアーキテクト）
> **評価カテゴリ**: {model_category}
> **ステータス**: 完了

---

## 目次

1. [エグゼクティブサマリ](#1-エグゼクティブサマリ)
2. [評価対象モデル](#2-評価対象モデル)
3. [ベンチマークスコア比較](#3-ベンチマークスコア比較)
4. [ライセンス・商用利用](#4-ライセンス商用利用)
5. [リソース使用量](#5-リソース使用量)
6. [コスト効率分析](#6-コスト効率分析)
7. [推奨モデル](#7-推奨モデル)
8. [参考情報](#8-参考情報)

---

## 1. エグゼクティブサマリ

### 推奨結論

**{use_case}には{推奨モデル}を推奨する。** {1-2文の推奨理由}

| 推奨順位 | モデル | 推奨理由 | 制約充足 |
|---------|--------|---------|---------|
| **1位** | {model_1} | {理由} | ◎ |
| **2位** | {model_2} | {理由} | ○ |
| **3位** | {model_3} | {理由} | ○ |

### 要件との適合性

| 要件 | 最適モデル |
|------|-----------|
| 精度重視 | {model} |
| コスト重視 | {model} |
| 日本語対応 | {model} |
| 低リソース | {model} |

---

## 2. 評価対象モデル

| モデル名 | 開発元 | パラメータ数 | 公開日 | HuggingFace |
|---------|--------|-------------|--------|-------------|
| {model_1} | {org} | {size} | {date} | [Link]({url}) |
| {model_2} | {org} | {size} | {date} | [Link]({url}) |
| ... | ... | ... | ... | ... |

### モデル概要

#### {model_1}
{概要説明 -- 特徴、設計思想、強み}

#### {model_2}
{概要説明}

---

## 3. ベンチマークスコア比較

### 3.1 総合ベンチマーク

| モデル | MTEB Avg | 検索 | 分類 | クラスタリング | 再順位付け |
|--------|----------|------|------|---------------|-----------|
| {model_1} | {score} | {score} | {score} | {score} | {score} |
| {model_2} | {score} | {score} | {score} | {score} | {score} |

> **データソース**: MTEB Leaderboard ({取得日})

### 3.2 日本語ベンチマーク（該当する場合）

| モデル | JMTEB Avg | 検索 | 分類 | STS |
|--------|-----------|------|------|-----|
| {model_1} | {score} | {score} | {score} | {score} |
| {model_2} | {score} | {score} | {score} | {score} |

### 3.3 ベンチマーク可視化

```
{ASCIIアートでのスコア比較バーチャート}

モデル別総合スコア（MTEB Avg）
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{model_1}  ████████████████████████████████████  68.5
{model_2}  ██████████████████████████████████    66.2
{model_3}  ████████████████████████████████      64.8
{model_4}  ██████████████████████████████        62.1
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 4. ライセンス・商用利用

| モデル | ライセンス | 商用利用 | 制約事項 | 確認URL |
|--------|-----------|---------|---------|---------|
| {model_1} | Apache-2.0 | ◎ 自由 | なし | [Link]({url}) |
| {model_2} | MIT | ◎ 自由 | なし | [Link]({url}) |
| {model_3} | Llama 3 | ○ 条件付き | MAU 7億未満 | [Link]({url}) |
| {model_4} | CC-BY-NC | ✗ 不可 | 研究目的のみ | [Link]({url}) |

### ライセンス詳細

#### Apache-2.0 / MIT（完全オープン）
- 商用利用: 自由
- 派生物作成: 可
- 特許許諾: あり（Apache-2.0）
- 帰属表示: 必要

#### Llama 3 Community License（条件付き）
- 商用利用: 月間アクティブユーザー7億人未満は可
- 大規模サービスでの利用: 要相談

---

## 5. リソース使用量

| モデル | パラメータ | VRAM (FP16) | VRAM (INT4) | 速度 | 次元数 | コンテキスト長 |
|--------|-----------|-------------|-------------|------|--------|---------------|
| {model_1} | 335M | 1.5GB | 0.5GB | 2000 tok/s | 1024 | 8K |
| {model_2} | 570M | 2.5GB | 1GB | 1500 tok/s | 1024 | 8K |
| {model_3} | 7B | 14GB | 4GB | 50 tok/s | 4096 | 32K |

### デプロイ要件別適合性

```
{ASCIIアートでのリソース比較}

VRAM使用量比較（FP16）
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{model_1}  ██                                    1.5GB
{model_2}  ████                                  2.5GB
{model_3}  ████████████████████████████          14GB
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
           |     |     |     |     |     |
           0    4GB   8GB  12GB  16GB  24GB
```

| デプロイ環境 | 適合モデル |
|-------------|-----------|
| エッジ（4GB VRAM） | {model_1}, {model_2} |
| ミッドレンジGPU（8GB） | {model_1}, {model_2}, {model_3} (INT4) |
| ハイエンドGPU（24GB+） | 全モデル対応 |

---

## 6. コスト効率分析

### 6.1 API利用コスト（該当する場合）

| モデル | 入力単価 | 出力単価 | 月間コスト概算* |
|--------|---------|---------|----------------|
| {model_1} (API) | $0.02/1M tok | $0.02/1M tok | $100 |
| {model_2} (API) | $0.05/1M tok | $0.05/1M tok | $250 |

*月間500万トークン想定

### 6.2 セルフホスティングコスト

| モデル | 推奨GPU | GPU時間単価 | 月間コスト概算* |
|--------|--------|------------|----------------|
| {model_1} | T4 | $0.35/hr | $252 |
| {model_2} | A10G | $1.00/hr | $720 |
| {model_3} | A100 | $3.00/hr | $2,160 |

*月間720時間稼働想定

### 6.3 コスト効率指数

| モデル | ベンチマーク | 月間コスト | コスト効率* | 評価 |
|--------|-------------|-----------|------------|------|
| {model_1} | 68.5 | $100 | 0.685 | ★★★★★ |
| {model_2} | 72.1 | $250 | 0.288 | ★★★☆☆ |
| {model_3} | 85.2 | $500 | 0.170 | ★★☆☆☆ |

*コスト効率 = ベンチマークスコア / 月間コスト（高いほど効率的）

---

## 7. 推奨モデル

### 7.1 ユースケース別推奨

| ユースケース | 推奨モデル | 理由 |
|-------------|-----------|------|
| **精度重視** | {model} | ベンチマーク最高スコア |
| **コスト重視** | {model} | コスト効率指数最高 |
| **日本語特化** | {model} | JMTEB最高スコア |
| **低リソース** | {model} | 1GB VRAM以下で動作 |
| **バランス型** | {model} | 精度・コスト・リソースのバランス良好 |

### 7.2 制約条件別推奨

#### 商用利用必須 + VRAM 8GB以下
**推奨: {model}**
- ライセンス: Apache-2.0（商用利用自由）
- VRAM: 2.5GB（FP16）/ 1GB（INT4）
- ベンチマーク: MTEB Avg 66.2

#### 日本語対応 + 精度重視
**推奨: {model}**
- JMTEB: 72.5（日本語ベンチマーク最高）
- MTEB: 68.5（英語も高精度）

### 7.3 総合推奨

```
┌─────────────────────────────────────────────────────────────┐
│                     総合推奨モデル                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  🥇 1位: {model_1}                                          │
│     推奨理由: {理由}                                         │
│     適用場面: {場面}                                         │
│                                                             │
│  🥈 2位: {model_2}                                          │
│     推奨理由: {理由}                                         │
│     適用場面: {場面}                                         │
│                                                             │
│  🥉 3位: {model_3}                                          │
│     推奨理由: {理由}                                         │
│     適用場面: {場面}                                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 7.4 導入ロードマップ

```
Phase 1: PoC（1-2週間）
├── 推奨モデル1位でプロトタイプ作成
├── 実データでの精度検証
└── レイテンシ・スループット測定

    │
    ▼

Phase 2: 本番導入準備（2-4週間）
├── インフラ構築（GPU/API設定）
├── モニタリング設定
└── フォールバック設計

    │
    ▼

Phase 3: 本番運用
├── 段階的トラフィック移行
├── A/Bテスト（既存モデルとの比較）
└── コスト・精度の継続監視
```

---

## 8. 参考情報

### ベンチマーク・リーダーボード

| リソース | URL | 概要 |
|---------|-----|------|
| MTEB Leaderboard | https://huggingface.co/spaces/mteb/leaderboard | Embeddingモデルベンチマーク |
| Open LLM Leaderboard | https://huggingface.co/open-llm-leaderboard | LLMベンチマーク |
| LMSYS Chatbot Arena | https://chat.lmsys.org/?leaderboard | LLM対話品質ランキング |

### モデルカード・ドキュメント

| モデル | HuggingFace | GitHub |
|--------|-------------|--------|
| {model_1} | [Link]({url}) | [Link]({url}) |
| {model_2} | [Link]({url}) | [Link]({url}) |

### 学術論文（該当する場合）

| 論文タイトル | URL | 概要 |
|-------------|-----|------|
| ... | https://arxiv.org/abs/... | ... |

---

*本レポートは{agent_name}がMLエンジニア / ソリューションアーキテクトとして調査・作成したものである。*
*ベンチマークスコアは{取得日}時点のデータに基づく。最新情報は各リーダーボードを参照されたい。*
```

## Examples

### Example 1: RAG vs MCP（補完関係 -- O-023実績ベース）

```
入力:
  technology_a: "RAG"
  technology_b: "MCP"
  output_path: "output/O-023_rag_mcp_analysis.md"
  context: "Nablarch開発支援"
  relationship_hypothesis: "complementary"
  depth: "thorough"
  use_cases:
    - "ハンドラキュー自動設計"
    - "コード生成"
    - "トラブルシューティング"
    - "学習支援"
    - "コードレビュー"
    - "マイグレーション支援"
    - "テスト生成"
  arxiv_search: true

Phase 1 調査結果:
  - 関係性: 補完関係と確定
    RAG = AIが「知る」ための仕組み
    MCP = AIが「使う」ための仕組み
    機能の重複: ほぼ0%
  - arXiv論文: arXiv:2505.03275（RAG-MCP）を発見
    定量結果: プロンプトトークン75%削減、精度3倍向上
  - アナロジー: 「図書館」メタファー
    RAG = 蔵書検索（知識の検索）
    MCP = 窓口サービス（機能の提供）

Phase 2-4 比較軸:
  - 10軸比較表を作成
  - 3つの統合パターンを図解
  - 意思決定フロー（検索精度 x ツール必要性の2軸）

Phase 5 ユースケース:
  - 7件のユースケースを分析（各Mermaidシーケンス図付き）
  - 貢献度マトリクス: RAGは全UCで★4以上、MCPはツール系UCで★4以上

Phase 6 推奨:
  - 「RAG-enhanced Nablarch MCPサーバー」を推奨
  - 4フェーズのロードマップ
  - 技術スタック: MCP Java SDK + pgvector + Jina v4 + Voyage-code-3

出力: 1190行のMarkdownレポート
  - Mermaid図: 12枚（シーケンス図7枚 + アーキテクチャ図5枚）
  - ASCIIアート図: 15枚以上
  - 参考文献: 18件（学術論文1件 + 技術ブログ5件 + 実装事例6件 + 既存資料5件 + 公式仕様3件）
```

### Example 2: React vs Vue.js（代替関係）

```
入力:
  technology_a: "React"
  technology_b: "Vue.js"
  output_path: "output/react_vs_vuejs_comparison.md"
  context: "フロントエンド技術選定"
  relationship_hypothesis: "alternative"
  depth: "standard"
  comparison_axes:
    - "TypeScript対応"
    - "状態管理"
    - "SSR/SSG"
  github_repos:
    technology_a:
      - owner: "facebook"
        repo: "react"
    technology_b:
      - owner: "vuejs"
        repo: "core"

Phase 1 調査結果:
  - 関係性: 代替関係（同一問題を異なるアプローチで解決）
    React = ライブラリアプローチ（UIレンダリングに特化、選択の自由）
    Vue.js = フレームワークアプローチ（公式エコシステム、統合体験）
  - 統合パターンは基本的に存在しない（併用は稀）

Phase 2-4 比較軸:
  - 11軸比較表（標準8軸 + TypeScript + 状態管理 + SSR/SSG）
  - GitHub統計: React 234k Stars vs Vue 47k Stars
  - 意思決定フロー（チーム規模 x 自由度要求の2軸）

Phase 5 ユースケース:
  - 5件: SPA構築、大規模エンタープライズ、プロトタイプ、既存jQueryからの移行、モバイルアプリ

出力: 700行のMarkdownレポート
```

### Example 3: Kubernetes vs Docker Compose（依存+進化関係）

```
入力:
  technology_a: "Docker Compose"
  technology_b: "Kubernetes"
  output_path: "output/compose_vs_k8s_comparison.md"
  context: "コンテナオーケストレーション選定"
  depth: "standard"
  use_cases:
    - "開発環境構築"
    - "本番デプロイ"
    - "マイクロサービス運用"
    - "CI/CDパイプライン"
    - "スケーリング"

Phase 1 調査結果:
  - 関係性: 依存関係（Docker → Kubernetes）かつ進化関係（Compose → K8s）
    Docker Compose = 単一ホストの開発環境向けオーケストレーション
    Kubernetes = マルチノードの本番環境向けオーケストレーション
  - 統合パターン: 開発環境=Compose、本番=K8s が典型
  - Kompose（ComposeファイルからK8sマニフェスト生成）の存在

出力: 650行のMarkdownレポート
```

### Example 4: Embeddingモデル比較（OSSモデル評価モード）

```
入力:
  evaluation_mode: "oss_model"
  model_category: "embedding"
  target_models:
    - "text-embedding-3-large"
    - "jina-embeddings-v3"
    - "voyage-3"
    - "bge-m3"
    - "multilingual-e5-large-instruct"
  use_case: "RAG検索（Nablarch技術ドキュメント）"
  language_requirements: "japanese"
  deployment_target: "on_premise"
  constraints:
    commercial_use: true
    max_vram_gb: 8
  output_path: "output/embedding_model_evaluation.md"

Phase M2 調査結果:
  - MTEB Leaderboard取得
    text-embedding-3-large: 64.6
    jina-embeddings-v3: 65.5
    voyage-3: 67.3
    bge-m3: 68.3
    multilingual-e5-large-instruct: 64.2
  - 日本語ベンチマーク（JMTEB）
    bge-m3: 72.5（日本語対応優秀）
    jina-embeddings-v3: 70.2

Phase M3 調査結果:
  - ライセンス
    jina-embeddings-v3: Apache-2.0 ◎
    bge-m3: MIT ◎
    multilingual-e5-large-instruct: MIT ◎
    text-embedding-3-large: 商用API（従量課金）
    voyage-3: 商用API（従量課金）

Phase M4 調査結果:
  - リソース使用量
    jina-embeddings-v3: 570M params, 2.5GB VRAM
    bge-m3: 568M params, 2.4GB VRAM
    multilingual-e5-large-instruct: 560M params, 2.3GB VRAM

Phase M5 コスト効率:
  - オンプレミス想定（T4 GPU月720時間）
    OSSモデル: $252/月（GPU代のみ）
  - API利用想定（月500万トークン）
    text-embedding-3-large: $65/月
    voyage-3: $60/月

Phase M6 推奨:
  1位: jina-embeddings-v3
    理由: Apache-2.0、日本語対応、8GB VRAM以下、MTEB上位
  2位: bge-m3
    理由: MIT、日本語ベンチマーク最高、軽量
  3位: multilingual-e5-large-instruct
    理由: MIT、低リソース、多言語対応

出力: 450行のモデル評価レポート
```

### Example 5: LLM比較（商用利用必須・低コスト重視）

```
入力:
  evaluation_mode: "oss_model"
  model_category: "llm"
  target_models:
    - "Llama-3.1-8B-Instruct"
    - "Mistral-7B-Instruct-v0.3"
    - "Qwen2.5-7B-Instruct"
    - "gemma-2-9b-it"
    - "Phi-3.5-mini-instruct"
  use_case: "コード生成・レビュー支援"
  language_requirements: "multilingual"
  deployment_target: "on_premise"
  constraints:
    commercial_use: true
    max_vram_gb: 24
    max_cost_per_month_usd: 1000
  output_path: "output/llm_code_assistant_evaluation.md"

Phase M2 調査結果:
  - Open LLM Leaderboard
    Qwen2.5-7B-Instruct: 74.2
    Llama-3.1-8B-Instruct: 72.8
    Mistral-7B-Instruct: 68.5
    gemma-2-9b-it: 71.2
    Phi-3.5-mini-instruct: 69.8
  - HumanEval（コード生成）
    Qwen2.5-7B-Instruct: 75.6%
    Llama-3.1-8B-Instruct: 72.1%

Phase M3 調査結果:
  - ライセンス
    Llama-3.1-8B: Llama 3.1 Community License（MAU 7億未満OK）
    Mistral-7B: Apache-2.0 ◎
    Qwen2.5-7B: Qwen License（商用利用可、出力利用制限なし）
    gemma-2-9b: Gemma Terms（商用利用可、一部制限）
    Phi-3.5-mini: MIT ◎

Phase M4 調査結果:
  - VRAM使用量（FP16）
    Phi-3.5-mini (3.8B): 8GB
    Mistral-7B: 14GB
    Qwen2.5-7B: 14GB
    Llama-3.1-8B: 16GB
    gemma-2-9b: 18GB

Phase M6 推奨:
  1位: Qwen2.5-7B-Instruct
    理由: コード生成ベンチマーク最高、商用利用可、24GB以内
  2位: Phi-3.5-mini-instruct
    理由: MIT、8GB VRAM、コスト効率最高
  3位: Mistral-7B-Instruct-v0.3
    理由: Apache-2.0、安定性、コミュニティ大

出力: 500行のLLM評価レポート
```

## Guidelines

### 必須ルール

1. **結論先行（エグゼクティブサマリ最優先）**
   - レポートの冒頭で結論を述べる
   - 忙しい読者がエグゼクティブサマリだけ読んでも判断できるようにする
   - 結論は1文で表現する: 「{技術A}と{技術B}は{関係性}にある。」

2. **公平性の確保**
   - 両技術の長所と短所を均等に記述する
   - 「技術Aは全面的に優れている」のような一方的な結論を避ける
   - 「場面による」は曖昧なので、具体的にどの場面で何が優れるか明記する
   - 統合パターンを過度に推奨しない（単体利用が適切な場面も記述する）

3. **図解の充実（最も重要な差別化要素）**
   - Mermaid図: 最低5枚（関係性1 + シーケンス3 + アーキテクチャ1）
   - ASCIIアート図: 最低3枚（各技術の構造 + 統合パターン）
   - 意思決定フローチャート: 1枚
   - 比較表: 最低2枚（メイン比較表 + 貢献度マトリクス）
   - 全ての図に説明テキストを付ける

4. **ソースの明記**
   - 全ての主要主張にソースURL or 論文引用を付ける
   - arXiv論文の引用フォーマット: `（{著者} et al., arXiv:{ID}）`
   - 推測と事実を明確に区別する（推測には「推測される」「可能性がある」と明記）
   - 定量データには必ず出典と取得日を付ける

5. **Mermaid構文の正確性**
   - 全てのMermaid図を書いた後、構文エラーがないか確認する
   - 特に注意すべきポイント:
     - `sequenceDiagram` の participant 宣言
     - `graph` の方向指定（LR, TD, BT, RL）
     - ノード名に特殊文字を含める場合は `""` で囲む
     - 日本語を含むラベルは `""` で囲むか、改行に `<br/>` を使用

6. **arXiv論文の活用**
   - 該当する学術論文が見つかった場合は必ず引用する
   - 定量結果（精度○%向上、コスト○%削減等）を具体的に引用する
   - 論文が見つからない場合は「学術文献での直接比較は見つからなかった」と明記する
   - 論文の結論を無批判に受け入れず、実験条件の制約も記述する

7. **並列実行を最大限活用する**
   - 技術AとBの検索は完全に独立 → 並列実行
   - 独立したWebSearchは全て並列実行（1フェーズで8-12件の並列実行も可）
   - 独立したWebFetchは4件まで並列実行
   - gh api呼び出しも並列実行可能

8. **レポートの最小行数を守る**
   - quick: 300行以上
   - standard: 600行以上
   - thorough: 900行以上
   - 行数不足の場合、ユースケースか図解を追加する

### 検索のコツ（実践知見）

1. **比較記事は英語圏が充実している**
   - 「{技術A} vs {技術B}」の英語記事が最も情報量が多い
   - 日本語記事は「{技術A} {技術B} 比較」で検索
   - 両方の言語で検索し、情報を統合する

2. **「{技術A} with {技術B}」で統合事例を発見**
   - 「vs」だけでなく「with」「and」「+」でも検索する
   - 統合事例が多い場合は補完関係の可能性が高い

3. **公式ドキュメントの比較ページを優先**
   - 一部の公式ドキュメントには競合との比較ページがある
   - 例: Vue.js公式の「Comparison with Other Frameworks」
   - 公式の比較はバイアスがあるが、正確な技術情報を含む

4. **Stack Overflowの質問から実務課題を発見**
   - 「{技術A} or {技術B}」のStack Overflow質問は実務者の悩みを反映
   - 回答のスコアが高いものは信頼性が高い

5. **年号を含めた検索で最新比較を発見**
   - 「{技術A} vs {技術B} {current_year}」で最新の比較記事を優先
   - 2年以上前の比較記事は現在のバージョンと乖離している可能性がある

### アンチパターン（避けるべきこと）

- **一方的な推奨**: 片方の技術を全面的に推奨し、もう片方を否定する
- **機能リスト比較のみ**: 「○○ができる/できない」の羅列に終始し、ユースケースベースの分析がない
- **図解の不足**: テキストのみの比較で視覚的な理解を助けない（最低8枚の図解が必要）
- **古い情報の引用**: 3年以上前の比較記事を現在の状況として引用する
- **推測と事実の混同**: 「○○と言われている」のような曖昧な記述で出典を示さない
- **統合パターンの過度な推奨**: 「常に両方使うべき」と結論づけ、単体利用の利点を無視する
- **抽象的な結論**: 「場面による」「ケースバイケース」で終わり、具体的な判断基準を示さない
- **Mermaid構文エラー**: 図を含めるがレンダリングできない壊れたMermaid構文
- **コピー感のある比較表**: 全軸で「○ vs ◎」のように一方が常に優勢な比較表（公平性の欠如）
- **逐次的な検索実行**: 並列可能な検索を1つずつ実行し、時間を浪費する
- **エグゼクティブサマリの欠落**: 結論を述べず、読者に全文を読ませる構成
- **ユースケースなしの抽象比較**: 具体的なシナリオなく機能の優劣のみを議論する

### OSSモデル評価モード固有のルール

1. **ベンチマークの信頼性確保**
   - 必ず公開リーダーボード（MTEB、LMSYS、Open LLM Leaderboard等）を引用する
   - ベンチマークの取得日を明記する（情報の鮮度が重要）
   - 単一ベンチマークに依存せず、複数の評価軸で比較する
   - ベンチマークの限界（実タスクとの乖離等）も言及する

2. **ライセンス情報の正確性**
   - 必ずHuggingFaceモデルカードまたは公式GitHubでライセンスを確認する
   - 「商用利用可」の場合も条件（MAU制限等）を詳細に記載する
   - ライセンス変更の可能性に言及する（モデルによっては過去に変更あり）

3. **リソース情報の実用性**
   - 理論値だけでなく、実測値（可能な場合）を優先する
   - 量子化（INT4, INT8, GGUF等）オプションを必ず検討する
   - 推論速度はハードウェア依存のため、比較条件を明記する

4. **推奨の透明性**
   - 推奨理由を明確に記述する（「なんとなく良さそう」は禁止）
   - 制約条件（ライセンス、リソース、コスト）を満たしているか明示する
   - 推奨順位の根拠を定量的に示す（可能な場合）

5. **公平性の確保（モデル評価特有）**
   - 特定ベンダー/組織のモデルを過度に推奨しない
   - OSSモデルと商用APIの両方を公平に比較する
   - モデルの弱点も隠さず記載する

6. **情報の鮮度管理**
   - MLモデルは進化が速いため、3ヶ月以上前の情報には注意を促す
   - 「本レポートはYYYY-MM-DD時点の情報に基づく」と明記する
   - 最新情報はリーダーボード参照を推奨する旨を記載する

### モデル評価時の検索のコツ

1. **HuggingFaceを最優先**
   - `site:huggingface.co {model_name}` でモデルカードを検索
   - モデルカードにはベンチマーク、ライセンス、使用方法が集約されている

2. **リーダーボードの直接参照**
   - WebFetchでリーダーボードページを直接取得
   - 検索結果より正確で最新のスコアが得られる

3. **GitHub Issuesで実運用の声を確認**
   - `{model_name} site:github.com issues` で実ユーザーの問題点を発見
   - ベンチマークでは見えない実運用上の課題がわかる

4. **Redditで実務者の評価を確認**
   - `{model_name} site:reddit.com r/LocalLLaMA` でLLMコミュニティの評価
   - `{model_name} site:reddit.com r/MachineLearning` で研究者視点の評価

5. **arXivで技術論文を確認**
   - モデルの技術的詳細（アーキテクチャ、学習データ等）は論文が最も正確
   - 特に新しいモデルは論文が唯一の詳細情報源の場合がある
